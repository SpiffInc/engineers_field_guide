# Getting Work Done

In order to make real progress on our application, we need to have a holistic understanding of the system.
Over time we will find ways to divide our overall product into separate systems so that developers can understand just one piece,
but in the early days our best bet is to understand as many of the pieces as we can.
Code review and Pair Programming are the two main tools we have for sharing our understanding.

## Pair Programming

Pair programming is not mandated at Spiff.
It is a tool we sometimes use in order to get help when we feel stuck or need to understand something better.
Sometimes we pair just because it's fun or we haven't had a chance to talk to someone recently.
Several of us at Spiff use [LiveShare for Visual Studio Code](https://code.visualstudio.com/blogs/2017/11/15/live-share) as a quick way to share an editor.
This works great for pairing with people who are remote, working from home, or just to let each of you use your own key bindings, but share the code.

While pair programming pay attention for cases where your pair misunderstood something in your code.
You are getting a chance to see through the eyes of someone who hasn't already been thinking about this and 1 month from now that will be you.
"What does this do?" is not a big problem, but "this works like this, right?" is a big red flag if they guessed wrong.

## Code Reviews

All changes to Spiff codebases should go through a pull request step.
Pull requests are a great time to understand what other people in the team are working on and how it might affect your work.
They are also a great time to ask questions.

> "I followed the pattern in UpdateUserMutation here, does anyone know why we do it this way?"

During code review we try to automate as much of the mundane feedback as possible.
Code formatting standards should be enforced by CI, rather than engineers spending time debating about it.
The person opening the pull request should write up a brief that explains why a change is being made.
Does it fix a bug? Part of a new feature? Refactoring?

The person reviewing a PR should focus on questions like:

* Does this change benefit users?
* Could I extend the behavior in this PR?
* Could we accomplish the requirements of the "why" with something simpler?

This type of code review can be time-consuming because you will need to understand the "why" of the changes before you can really answer these questions.
You might need to follow a few links or talk to the product team.
This is exactly why we want human beings focused on these questions and why we push for automation of things that can be automated.

Projects at Spiff are generally setup to automatically deploy when we merge to master.
The project README should specify if this is not the case.

## The Hippocratic Oath of the Shipit

When you are reviewing code from a teammate, it's your responsibility to protect and increase the quality of our system.
Much like the [Hippocratic Oath](https://en.wikipedia.org/wiki/Hippocratic_Oath), we need to "first do no harm".
Specifically you should consider the following types of harm:

* Does no harm to users?
  * Does this change compromise the privacy of our users or customers?
  * Does this change make it harder to get things done for our users?
  * Will users be surprised or confused by these changes?
* Does no harm to our platform?
  * Will this change cause unstable memory usage? Does it have a query that will cause a problems for the production database?
  * When this feature breaks, what will the error message look like? How hard will it be to troubleshoot the problem?
  * Should we track any metrics around these changes? Do we need observability tools?
* Does no harm to the project?
  * Is this code understandable for a new engineer 3 months from now?
  * Would the project be better if we changed the structure of the code?

If you can't answer all of the questions above with confidence, consider leaving a comment along the lines of:

> Looks good in terms of code quality, but I'm not sure about performance. Have we done any benchmarks against production data?

> This is going to be great for the onboarding team❤️. Is it safe for us to change this part of the graphQL? Or should we add a new field so the frontend code can switch to it in a backwards-compatible way?

